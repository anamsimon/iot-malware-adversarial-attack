The CICIoT2023 has proposed a novel and extensive IoT attack dataset by executing 33 attacks in an IoT topology composed of 105 devices. The attacks are classified into attack/benign category and other seven categories DDoS, DoS, Recon, Web-based, brute force, spoofing, and Mirai. 

## The CICIoT2023 dataset
The dataset contains 642,635 records with following features extracted - flow_duration, Header_Length, Protocol Type, Duration, Rate, Srate, Drate, fin_flag_number, syn_flag_number, rst_flag_number, psh_flag_number, ack_flag_number, ece_flag_number, cwr_flag_number, ack_count, syn_count, fin_count, urg_count, rst_count, HTTP, HTTPS, DNS, Telnet, SMTP, SSH, IRC, TCP, UDP, DHCP, ARP, ICMP, IPv, LLC, Tot sum, Min, Max, AVG, Std, Tot size, IAT, Number, Magnitue, Radius, Covariance, Variance, Weight and label.
The way the data are processed makes it easier for researchers to access the information and then an evaluation using machine learning (ML) is done to demonstrate how the suggested data set can be used to its full potential for classification. They have evaluated ML performance in three ways: (i) multiclass classification of 33 individual attacks; (ii) grouped classification of 7 attacks(e.g., DDoS and DoS); and (iii) binary classification (i.e., malicious, and benign traffic classification) dividing the dataset into the train (80%) and test (20%) sets, which are normalized using the StandardScaler method before the actual training process. 

In our research, we rerun their ML model on the CICIoT2023. dataset then generated adversarial examples and performed the attack on the model. 

## The Adversarial Attack
In machine learning, adversarial attack is the deliberate manipulation of input data to a machine learning model with the intention of causing the model to make incorrect predictions or classifications. The goal of such attacks is to exploit vulnerabilities in the model and reveal weaknesses in its decision-making process. Adversarial attacks can be a significant concern, especially in critical applications such as security systems, autonomous vehicles, and medical diagnosis.
From the attack environment, the adversarial attack can be divided into the white-box attack and the black-box attack. The white-box attack means that the adversary can use the pre-knowledge of the internal structure of the model and gradient information to prepare the examples. 

Fast Gradient Sign Method (FGSM)
Gradient-based techniques, which use the network's loss gradient to determine the assault direction, are one type of white-box attack strategy. It is a classical white-box attack method, as follows:
 
adv_x = x + ϵ * sign (VxJ(θ,x,y))
 
Where,
	adv_x : Adversarial data
	x : Original input data
	y : Original input label
	ϵ : Multiplier to ensure the perturbations are small.
	θ: Model parameters.
	J: Loss.
ε is the multiplier to ensure the perturbations are small.

## Adversarial Dataset Generation

Before adversarial data generation, we have scaled and standardized the dataset using fit and fit_transform method. We have mapped the output labels like DDoS-RSTFINFlood, DDoS-PSHACK_Flood, DDoS-SYN_Flood etc into Attack and BenignTraffic into Benign. After labeling, we have generated adversarial data from the original test set X using different epsilon ϵ values 0.1, 0.2, 0.3 and 0.5 to get 4 sets of adversarial data sets.

##  Adversarial Attack & Results 
We run the prediction function of the Logistic Regression model with the original test set X and with the 4 adversarial datasets.  Accuracy is then calculated on the result of each adversarial data set and compared with the original prediction accuracy.
From the experiment we find that with the small value of epsilon ϵ like 0.1, 0.2 the result accuracy does not vary much compared with original accuracy. As we increase the ϵ value from 0.3 to 0.5 the accuracy drops drastically to 0.761964 to 0.35968.

## Future Research Direction
After the adversarial attack the next research could be done in model training with the adversarial data to make the model defend against the attack. Our research focused on the white box attack area where we had access to the model coefficients and data sets. 

Further research can be done in the black box attack area where we generate a dataset X’ and train a model M’ without the knowledge of the existing trained model M of data set X. Then see if we can use  X’ dataset to exploit the model M.


Reference: Neto, E.C.P.; Dadkhah, S.; Ferreira, R.; Zohourian, A.; Lu, R.; Ghorbani, A.A. CICIoT2023: A Real-Time Dataset and Benchmark for Large-Scale Attacks in IoT Environment. Sensors 2023, 23, 5941. https://doi.org/10.3390/s23135941.
